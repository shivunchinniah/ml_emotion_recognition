{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a72127b",
   "metadata": {},
   "source": [
    "# Lab instruction: Model evaluation\n",
    "Model evaluation forms an important part of the development of machine learning models as this allows us to estimate the performance of the model and to test what is the expected performance upon deployment. In order to achieve such evaluation we employ *metrics*, which are measurements that incdicates us aspects related to the performance of the model.\n",
    "\n",
    "# Measuring performance\n",
    "\n",
    "In order to evaluate the performance of a classifier, there are different metrics that can help us. Simple examples of them are the following:\n",
    "\n",
    "* The recall/precision/true positive rate measures teh proportion of which of the true positives were detectted with respect to all samples of the positive class. The recall is the  is defined by the expression\n",
    "\n",
    "Recall = (True Positives) / (True Positives + False Negatives)\n",
    "\n",
    "* The precision describes which of the samples predicted as positive class are actual positive class members. It is expressed by \n",
    "\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "* A major problem of the precision and recall is that they suffer under class imbalance. For example, if we had 99% of samples of the positive class and 1% of the negative class, then the precision would be 99% even if we missclassify all the negative class samples. To account for this there are different measurements. For example, the  the F1 score is a\n",
    "\n",
    "F1 Score=2*(Precision * Recall)/(Precision + Recall)\n",
    "\n",
    "* Besides the F1 score, we can also use the balanced accuracy\n",
    "\n",
    "Balanced accuracy = (True positive rate + True negative rate)/2\n",
    "\n",
    "Where the True positive rate and the True negative rate is defined by\n",
    "\n",
    "True positive rate = (True positives)/(True Positives + False negatives)\n",
    "\n",
    "True negative rate = (True negatives)/(True negatives + False positives)\n",
    "â€‹\n",
    " \n",
    "### Excercise 1: Accuracy and balanced accuracy\n",
    "For this excercise you can test the difference between the accuracy and balanced accuracy. To test this, we will use the make_classification routine from sklearn.datasets. With this function we can generate an imbalanced dataset by changing the weights function. Each weight define the percentage of datapoints generated for each class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01882f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_classes=2 ,n_samples=3000, n_features=30, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train MLP\n",
    "model = MLPClassifier(random_state=42, validation_fraction=0.25, max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# The standard masurements for performance, such as the precision, recall and F1 scores per class (1 versus all) can be computed with the  \n",
    "# Sklearn's classification_report function\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7b9c1",
   "metadata": {},
   "source": [
    "Now that we computed the precision, recall adn f1 scores with Sklearn's classification_report. This function only classifies the regular accuracy, but does not compute the balanced accuracy score, which actually accounts for class imbalance. In order to improve this score, we can use the balanced_accuracy_score. Note that the balanced accuracy is lower than the balanced accuracy. \n",
    "\n",
    "What happens if you generate a balanced dataset in the previous cell with the accuracy and balanced accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96be488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# After computing the cla\n",
    "# Computing (un)balanced accuracy scores\n",
    "acc = accuracy_score(y_pred, y_test)\n",
    "bacc = balanced_accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(\"Accuracy: %1.2f, Balanced accuracy: %1.2f\"%(acc, bacc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b984bbf",
   "metadata": {},
   "source": [
    "### Exercise 2: Receiver Operating Characteristic (ROC) curve \n",
    "A different way to compute performance in classifiers is the ROC curve. This curve allows us to determine not only how good is a classifier. But also, for example, to define the threshold to be used for classification to tune our classifier to have specific true positive and false positive rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccf7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train and evaluate on validation set\n",
    "model = MLPClassifier(max_iter=2000, early_stopping=False, validation_fraction=0.25, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "# model.predict() generates a binary label, we need to get the raw output of the classifier instead\n",
    "# predict_proba() geneartes a continous label. This is what we need for the ROC curve\n",
    "y_pred = model.predict(X_test)\n",
    "y_score = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfba4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Here we generate the ROC curve!\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_score[:,1],\n",
    "    name=f\"ROC curve of MLP Classifier\",\n",
    "    curve_kwargs=dict(color=\"darkorange\"),\n",
    "    plot_chance_level=True,\n",
    "    despine=True,\n",
    ")\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"ROC curve\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22782442",
   "metadata": {},
   "source": [
    "Besides the ROC Curve, a common apprach to validate a model is the *area* under the ROC curve. This metric is ofen referred as the AUROC score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c74119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"AUROC for the MLP classifier: %1.2f\"%(roc_auc_score(y_test, y_score[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe37e44",
   "metadata": {},
   "source": [
    "Finaly, just as performned in the past, we can use the confusioon matrix also to evaluate the performance of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd326c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "_ = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdda9a3",
   "metadata": {},
   "source": [
    "# Exercise 3: Multiclass examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8bcafa",
   "metadata": {},
   "source": [
    " ### Cross validation for model selection\n",
    "Now that we explored the cross validatio, we can use Sklearn's GridSearchCV to define a grid search to find the best classifier model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and splitting the data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\n",
    "X, y = data, labels\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96174f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 25 digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(5, 5)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axs[i,j].imshow(X_train[i+j*5,:].reshape(8,8) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP\n",
    "model = MLPClassifier(random_state=42, validation_fraction=0.25, max_iter=2000, early_stopping=True, hidden_layer_sizes=(100))\n",
    "model.fit(X_train, y_train)\n",
    "_ = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668caced",
   "metadata": {},
   "source": [
    "Now it's your turn! Make the ROC curves for each class of your digit classifier. For this follow a one versus rest apprach (OvR). For this will need to one-hot encode the labels of the data with the sklearn.preprocessing LabelBinarizer. Afterwards just call for each class the computation of the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2p8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
